{"cells":[{"cell_type":"markdown","source":["# Data Quality Framework\n","\n","This notebook runs data quality tests defined in a YAML configuration file **in parallel** using Spark SQL.  \n","Each test executes as a separate concurrent task, speeding up validation across multiple tables and checks.\n","\n","Tests supported include:\n","- Row count minimum\n","- Null checks on specified columns\n","- Minimum and maximum value checks on columns\n","\n","Results are collected and displayed with pass/fail status and metric values.\n","\n","### Example YAML Config Structure (`checks.yml`)\n","\n","```yaml\n","tables:\n","  - name: silver.customers\n","    tests:\n","      - type: row_count\n","        min: 1\n","\n","      - type: not_null\n","        column: customer_id\n","\n","      - type: min_value\n","        column: total_amount\n","        min: 0\n","\n","      - type: max_value\n","        column: total_amount\n","        max: 10000\n","\n","  - name: silver.orders\n","    tests:\n","      - type: row_count\n","        min: 1\n","\n","      - type: not_null\n","        column: order_id\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b1e5fcb-1d25-40ca-97c5-3e31907dd0c9"},{"cell_type":"code","source":["debug=True\n","yaml_file_path=\"/lakehouse/default/Files/Tests/checks.yml\"\n","parallelism=5"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8fef4aa3-5a86-43b6-863e-fb520c8621be"},{"cell_type":"code","source":["import yaml\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","# Load your config\n","with open(yaml_file_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","\n","results = []\n","\n","def run_test(table_cfg, test):\n","    table = table_cfg[\"name\"]\n","\n","    if test[\"type\"] == \"row_count\":\n","        sql = f\"SELECT COUNT(*) AS cnt FROM {table}\"\n","        count = spark.sql(sql).collect()[0][\"cnt\"]\n","        passed = count >= test[\"min\"]\n","        return (table, \"row_count\", passed, count)\n","\n","    elif test[\"type\"] == \"not_null\":\n","        col = test[\"column\"]\n","        sql = f\"SELECT COUNT(*) AS cnt_nulls FROM {table} WHERE {col} IS NULL\"\n","        null_count = spark.sql(sql).collect()[0][\"cnt_nulls\"]\n","        passed = null_count == 0\n","        return (table, f\"not_null_{col}\", passed, null_count)\n","\n","    elif test[\"type\"] == \"min_value\":\n","        col = test[\"column\"]\n","        sql = f\"SELECT MIN({col}) AS min_val FROM {table}\"\n","        min_val = spark.sql(sql).collect()[0][\"min_val\"]\n","        passed = min_val >= test[\"min\"]\n","        return (table, f\"min_value_{col}\", passed, min_val)\n","\n","    elif test[\"type\"] == \"max_value\":\n","        col = test[\"column\"]\n","        sql = f\"SELECT MAX({col}) AS max_val FROM {table}\"\n","        max_val = spark.sql(sql).collect()[0][\"max_val\"]\n","        passed = max_val <= test[\"max\"]\n","        return (table, f\"max_value_{col}\", passed, max_val)\n","\n","    else:\n","        return (table, f\"unknown_test_type_{test['type']}\", False, None)\n","\n","# Create a ThreadPoolExecutor with as many workers as you want (e.g. 5)\n","with ThreadPoolExecutor(max_workers=5) as executor:\n","    future_to_test = {}\n","    for table_cfg in config[\"tables\"]:\n","        for test in table_cfg[\"tests\"]:\n","            future = executor.submit(run_test, table_cfg, test)\n","            future_to_test[future] = (table_cfg[\"name\"], test)\n","\n","    for future in as_completed(future_to_test):\n","        result = future.result()\n","        results.append(result)\n","\n","# Show results\n","if debug:\n","    for r in results:\n","        table, test_name, passed, val = r\n","        print(f\"{table} - {test_name} => {'PASS' if passed else 'FAIL'} (Value: {val})\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4c3a0644-bccf-44cf-8693-a4497dfb06f5"},{"cell_type":"code","source":["from pyspark.sql import Row\n","from pyspark.sql.types import StructType, StructField, StringType, BooleanType, DoubleType, TimestampType\n","import pyspark.sql.functions as F\n","\n","# Define the schema explicitly\n","schema = StructType([\n","    StructField(\"table_name\", StringType(), nullable=False),\n","    StructField(\"test_name\", StringType(), nullable=False),\n","    StructField(\"passed\", BooleanType(), nullable=False),\n","    StructField(\"value\", DoubleType(), nullable=True),\n","    StructField(\"run_timestamp\", TimestampType(), nullable=False),\n","])\n","\n","# Prepare rows with run_timestamp\n","import datetime\n","now = datetime.datetime.now()\n","\n","rows = [\n","    Row(\n","        table_name=r[0],\n","        test_name=r[1],\n","        passed=r[2],\n","        value=float(r[3]) if r[3] is not None else None,\n","        run_timestamp=now\n","    )\n","    for r in results\n","]\n","\n","# Create DataFrame with the schema\n","results_df = spark.createDataFrame(rows, schema=schema)\n","\n","# Write to Delta table\n","results_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"dbo.data_quality_results\")\n","\n","\n","if debug:\n","    # Show results\n","    results_df.show(truncate=False)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9b9a9756-d14c-4f70-b807-348492ef6ead"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"456dff82-ce1a-41bc-884d-54713ad64833"}],"default_lakehouse":"456dff82-ce1a-41bc-884d-54713ad64833","default_lakehouse_name":"LH_Umwelt","default_lakehouse_workspace_id":"5b5b42e3-8819-4378-bf66-53ccd7e3de8c"}}},"nbformat":4,"nbformat_minor":5}